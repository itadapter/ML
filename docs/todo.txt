
***************************** CURRENT *****************************

- CNN:
  - for Batch CPU parallelization
  - stochastic gradient training
  - [DONE] batch learning
  - batch normalization https://arxiv.org/pdf/1502.03167.pdf
  - [DONE] dropout
  - [DONE] add activation layer
  - GPU parallelization
  - Read training set from hard drive not in-memory!
  - extract stop criteria from backpropalgorithm
  - [DONE] split 'size' into 'height' and 'width' in all layers
  - serialization/deserialization
  - compare Keras and ML calcuations

- COMMIT A WILLFUL DECISION: do we need ComputingLayer architecture? Does Build/DoBuild implenter in a proper way?

- NN and CNN:
  - possibility of: train -> add nodes/layers -> train etc...
  - possibility to remove layer/node from network

- implement more NN optimizatiors (https://habrahabr.ru/post/318970/)
  - Nesterov

- Unit tests
  - fix NN tests
  - cover CNN with tests
  - cover NN Backprops with tests
  - cover CNN Backprop with tests

- implement different CNN architectures
  - [FUTURE] LeNet-5
  - AlexNet
  - U-Net
  - test with CIFAR-10 images db: http://www.cs.toronto.edu/~kriz/cifar.html

- implement Backpropagation algorithm for general full-connected NN
  - [DONE] implement algorithm
  - [DONE] add start weight randomization
  - [DONE] add events to NN algorithms
  - add stochastic gradient descent
  - [DONE] add batching

- [DONE] add neuron NN + unit tests

- [DONE] neuron UseBias -> explicit neuron.BiasWeight value

- [DONE] Allwaye UseBias. Get rid of use_bias! fix tests

- integration tests: use data files

- demos project (+data samples)

- implement RBF algorith (https://en.wikipedia.org/wiki/Radial_basis_function_network, http://mccormickml.com/2013/08/15/radial-basis-function-network-rbfn-tutorial/)


- [INPROG] Play with ACCORD Framework
  - [INPROG] compare with ML (write benchmarks in .Tests assembly)

- add ability to update parameters in a batch from some start index (extend TryUpdateParams)

- General NN architecture
  - Read https://tproger.ru/translations/neural-network-zoo-1/ - NN zoo
  - add ability to loop nodes
  - develop way to create simple base networks (factory methods)
  - add network-wide global context
  - review architecture: public/internal members - try to use assembly from ouside

 - General ML architcture review
   - review/rewrite Algorithm architecture
     (may be extract Result:TModel property in AlgorithmBase,
      general Train() method that fills the Result in protected abstract DoTrain() method)

 - Cover all algorithm logic with unit tests

 - Implement more algorithms
   - implement C4.5 algorithm for decision trees

***************************** DONE *****************************

- implement more NN optimizatiors (https://habrahabr.ru/post/318970/)
  - Momentum
  - Adagrad
  - RMSProp
  - Adadelta
  - Adam
  - Adamax

- implement Convolution NN based on CN architecture
  - read more about Convolution NN
  - implement base CNN layer types
  - implement CNN
  - implement learning algorithm for NN
  - testing with real images
      - MNIST images db: http://yann.lecun.com/exdb/mnist/index.html, https://web.archive.org/web/20160117040036/http://yann.lecun.com/exdb/mnist/)

- implement Perceptron algorithm (no hidden layers)
  - refactor UseBias ( -> neurons threshold as additional parameter?) + cover with tests
  - Perceptron algorithm
  - backpropagation learning
  - batch learning

- Computing Networks/Layers: develop unified approach to multi-typed layers NN: Layer<TIn, TOut>
  - develop architecture
  - refactor: index
  - cover with unit tests
  - develop benchmark tests on parameters get/set/bulk
  - refactor: extract abstract base Layer class
    (may be just mix Hidden and Output logic with 'if' statement? - so there will be only 2 classes: ComputingNetwork and CompositeNetwork)
  - do we REALLY need linked-list architecture? Should we use more evident layered array architecture (with reflection type checks on AddLayer())?

- rewrite all NN code according to new CN architecture
  - rewrite NN
  - cover with tests
  - FlatNeuron and SparseNeuron added to support full-conencted NN along with the sparse ones

