CURRENT:

- [INPROG] implement Convolution NN based on CN architecture
  - [INPROG] read more about Convolution NN
  - [INPROG] implement base CNN layer types
  - implement CNN
  - implement learning algorithm for NN
  - testing with real images
      - MNIST images db: http://yann.lecun.com/exdb/mnist/index.html, https://web.archive.org/web/20160117040036/http://yann.lecun.com/exdb/mnist/)
      - CIFAR-10 images db: http://www.cs.toronto.edu/~kriz/cifar.html
  - cover with tests

- implement Backpropagation algorithm for general full-connected NN
  - [DONE] implement algorithm
  - [DONE] add start weight randomization
  - [DONE] add events to NN algorithms
  - add stochastic gradient descent
  - add batching (+stochastic)

- [DONE] add neuron NN + unit tests

- [DONE] neuron UseBias -> explicit neuron.BiasWeight value

- [DONE] Allwaye UseBias. Get rid of use_bias! fix tests

- integration tests: use data files

- demos project (+data samples)

- implement RBF algorith (https://en.wikipedia.org/wiki/Radial_basis_function_network, http://mccormickml.com/2013/08/15/radial-basis-function-network-rbfn-tutorial/)

- implement more NN algorithms (https://habrahabr.ru/post/318970/)
  - Nesterov
  - Adagrad
  - RMSProp
  - Adadelta
  - Adam
  - Adamax

- [INPROG] Play with ACCORD Framework
  - [INPROG] compare with ML (write benchmarks in .Tests assembly)

- add ability to update paraters in a batch from some start index (extend TryUpdateParams)

- General CN architecture
  - Read https://tproger.ru/translations/neural-network-zoo-1/ - NN zoo
  - add ability to loop nodes
  - develop way to create simple base networks (factory methods)
  - add network-wide global context
  - review architecture: public/internal members - try to use assembly from ouside

 - General ML architcture review
   - review/rewrite Algorithm architecture
     (may be extract Result:TModel property in AlgorithmBase,
      general Train() method that fills the Result in protected abstract DoTrain() method)

 - Cover all algorithm logic with unit tests

 - Implement more algorithms
   - implement C4.5 algorithm for decision trees

DONE:

- implement Perceptron algorithm (no hidden layers)
  - refactor UseBias ( -> neurons threshold as additional parameter?) + cover with tests
  - Perceptron algorithm
  - backpropagation learning
  - batch learning

- Computing Networks/Layers: develop unified approach to multi-typed layers NN: Layer<TIn, TOut>
  - develop architecture
  - refactor: index
  - cover with unit tests
  - develop benchmark tests on parameters get/set/bulk
  - refactor: extract abstract base Layer class
    (may be just mix Hidden and Output logic with 'if' statement? - so there will be only 2 classes: ComputingNetwork and CompositeNetwork)
  - do we REALLY need linked-list architecture? Should we use more evident layered array architecture (with reflection type checks on AddLayer())?

- rewrite all NN code according to new CN architecture
  - rewrite NN
  - cover with tests
  - FlatNeuron and SparseNeuron added to support full-conencted NN along with the sparse ones

