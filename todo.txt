CURRENT:

- [INPROG] implement Perceptron algorithm according to new CN architecture
  - [DONE] refactor UseBias ( -> neurons threshold as additional parameter?) + cover with tests
  - [DONE] Perceptron algorithm
  - [DONE] backpropagation learning (+ play with public/private props/fields)
  - [DONE] batch learning
  - all above for 1 Hidden Layer NN
  - cover with tests

- implement Gradient Descent algorithm for general NN
  - with back propagation
  - with stochastic
  - cover with tests

- [INPROG] Play with ACCORD Framework
  - [INPROG] compare with ML (write benchmarks in .Tests assembly)

- General CN architecture
  - Read https://tproger.ru/translations/neural-network-zoo-1/ - NN zoo
  - add ability to loop nodes
  - develop way to create simple base networks (factory methods)
  - add network-wide global context
  - review architecture: public/internal members - try to use assembly from ouside

- implement Convolution NN based on CN architecture
  - read more about Convolution NN
  - implement base CNN layer types
  - implement CNN
  - implement learning algorithm for NN
  - testing with real images
  - cover with tests

 - General ML architcture review
   - review/rewrite Algorithm architecture
     (may be extract Result:TModel property in AlgorithmBase,
      general Train() method that fills the Result in protected abstract DoTrain() method)

 - Cover all algorithm logic with unit tests

 - Implement more algorithms
   - implement C4.5 algorithm for decision trees

DONE:

- Computing Networks/Layers: develop unified approach to multi-typed layers NN: Layer<TIn, TOut>
  - develop architecture
  - refactor: index
  - cover with unit tests
  - develop benchmark tests on parameters get/set/bulk
  - refactor: extract abstract base Layer class
    (may be just mix Hidden and Output logic with 'if' statement? - so there will be only 2 classes: ComputingNetwork and CompositeNetwork)
  - do we REALLY need linked-list architecture? Should we use more evident layered array architecture (with reflection type checks on AddLayer())?

- rewrite all NN code according to new CN architecture
  - rewrite NN
  - cover with tests
  - FlatNeuron and SparseNeuron added to support full-conencted NN along with the sparse ones

